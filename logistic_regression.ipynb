{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "iris = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\n",
    ")\n",
    "iris = iris[[\"species\", \"sepal_length\", \"sepal_width\"]]\n",
    "iris = iris[iris.species != \"virginica\"]\n",
    "iris = iris.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot using pandas\n",
    "ax = iris[iris.species == \"setosa\"].plot.scatter(\n",
    "    x=\"sepal_length\", y=\"sepal_width\", color=\"red\", label=\"setosa\"\n",
    ")\n",
    "iris[iris.species == \"versicolor\"].plot.scatter(\n",
    "    x=\"sepal_length\", y=\"sepal_width\", color=\"green\", label=\"versicolor\", ax=ax\n",
    ")\n",
    "ax.set_title(\"Scatter Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to classify between the two species \"setosa\" and \"versicolor\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression viewed as a Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression has two trainable parameters: a weight $W$ and a bias $b$. For a vector of features $X$, the prediction of logistic regression is given by\n",
    "\n",
    "$$\n",
    "f(X) = \\frac{1}{1 + \\exp(-[XW + b])} = \\sigma(h(X))\n",
    "$$\n",
    "where $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ and $h(X)=XW + b$.\n",
    "\n",
    "Parameters $W$ and $b$ are fitted by maximizing the log-likelihood (or minimizing the negative log-likelihood) of the model on the training data. For a training subset $\\{X_j, Y_j\\}_{j=1}^N$ the normalized negative log likelihood (NLL) is given by \n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{N}\\sum_j \\log\\Big[ f(X_j)^{Y_j} \\cdot (1-f(X_j))^{1-Y_j}\\Big]\n",
    "= -\\frac{1}{N}\\sum_j \\Big[ Y_j\\log f(X_j) + (1-Y_j)\\log(1-f(X_j))\\Big]\n",
    "$$\n",
    "\n",
    "The following algorithm is used for the **forward** pass:\n",
    "\n",
    "1. Linear mapping: $h=XW + b$\n",
    "2. Sigmoid activation function: $f=\\sigma(h)$\n",
    "3. Calculation of NLL: $\\mathcal{L} = -\\frac{1}{N}\\sum_j \\Big[ Y_j\\log f_j + (1-Y_j)\\log(1-f_j)\\Big]$\n",
    "\n",
    "In order to fit $W$ and $b$ we perform Gradient Descent. We choose a small learning rate $\\gamma$ and after each computation of forward pass, we update the parameters \n",
    "\n",
    "$$W_{\\text{new}} = W_{\\text{old}} - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial W}$$\n",
    "\n",
    "$$b_{\\text{new}} = b_{\\text{old}} - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial b}$$\n",
    "\n",
    "We use Backpropagation method to calculate the partial derivatives of the loss function with respect to the parameters of the model.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial W} = \n",
    "\\frac{\\partial\\mathcal{L}}{\\partial h} \\frac{\\partial h}{\\partial W} =\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial f} \\frac{\\partial f}{\\partial h} \\frac{\\partial h}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial b} = \n",
    "\\frac{\\partial\\mathcal{L}}{\\partial h} \\frac{\\partial h}{\\partial b} =\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial f} \\frac{\\partial f}{\\partial h} \\frac{\\partial h}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(x_input, W, b):\n",
    "    output = np.matmul(x_input, W) + b\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_grad_W(x_input, grad_output, W, b):\n",
    "    grad_W = np.matmul(x_input.T, grad_output)\n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_grad_b(x_input, grad_output, W, b):\n",
    "    grad_b = np.sum(grad_output.T)\n",
    "    return grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_forward(x_input):\n",
    "    output = 1 / (1 + np.exp(-x_input))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of the partial derivative of the loss function with respect to the input of sigmoid. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial f}\n",
    "\\frac{\\partial f}{\\partial h} \n",
    "$$\n",
    "\n",
    "Tensor $\\frac{\\partial \\mathcal{L}}{\\partial f}$ comes from the loss function. Let's calculate $\\frac{\\partial f}{\\partial h}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial h} = \n",
    "\\frac{\\partial \\sigma(h)}{\\partial h} =\n",
    "\\frac{\\partial}{\\partial h} \\Big(\\frac{1}{1 + e^{-h}}\\Big)\n",
    "= \\frac{e^{-h}}{(1 + e^{-h})^2}\n",
    "= \\frac{1}{1 + e^{-h}} \\frac{e^{-h}}{1 + e^{-h}}\n",
    "= f(h) (1 - f(h))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad_input(x_input, grad_output):\n",
    "    grad_input = sigmoid_forward(x_input) * (1 - sigmoid_forward(x_input)) * grad_output\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_forward(target_pred, target_true):\n",
    "    assert len(target_pred) == len(target_true)\n",
    "    s = np.sum(\n",
    "        [\n",
    "            j * np.log(i) + (1 - j) * np.log(1 - i)\n",
    "            for i, j in zip(target_pred, target_true)\n",
    "        ]\n",
    "    )\n",
    "    output = -1 / len(target_true) * s\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_grad_input(target_pred, target_true):\n",
    "    grad_input = (\n",
    "        1\n",
    "        / len(target_pred)\n",
    "        * (target_pred - target_true)\n",
    "        / (target_pred * (1 - target_pred))\n",
    "    )\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogsticRegressionGD(object):\n",
    "    def __init__(self, n_in, lr=0.05):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.b = np.zeros(1)\n",
    "        self.W = np.random.randn(n_in, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h = linear_forward(x, self.W, self.b)\n",
    "        y = sigmoid_forward(self.h)\n",
    "        return y\n",
    "\n",
    "    def update_params(self, x, nll_grad):\n",
    "        # compute gradients\n",
    "        grad_h = sigmoid_grad_input(self.h, nll_grad)\n",
    "        grad_W = linear_grad_W(x, grad_h, self.W, self.b)\n",
    "        grad_b = linear_grad_b(x, grad_h, self.W, self.b)\n",
    "        # update params\n",
    "        self.W = self.W - self.lr * grad_W\n",
    "        self.b = self.b - self.lr * grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the model to the Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode species names\n",
    "# setosa == 1 versicolor == 0\n",
    "iris.loc[iris[\"species\"] == \"setosa\", \"species\"] = 1\n",
    "iris.loc[iris[\"species\"] == \"versicolor\", \"species\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "# Prepare the data\n",
    "X = np.array(iris.drop([\"species\"], axis=1))\n",
    "y = np.array(np.matrix(iris[\"species\"]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogsticRegressionGD(2, 0.05)\n",
    "number_of_itterations = 30000\n",
    "\n",
    "for step in range(number_of_itterations):\n",
    "    y_pred = model.forward(X)\n",
    "\n",
    "    loss_value = nll_forward(y_pred, y)\n",
    "    accuracy = ((y_pred > 0.5) == y).mean()\n",
    "    loss_grad = nll_grad_input(y_pred, y)\n",
    "    model.update_params(X, loss_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the result\n",
    "def plot_model_prediction(prediction_func, X, Y):\n",
    "    u_min = X[:, 0].min() - 1\n",
    "    u_max = X[:, 0].max() + 1\n",
    "    v_min = X[:, 1].min() - 1\n",
    "    v_max = X[:, 1].max() + 1\n",
    "\n",
    "    U, V = np.meshgrid(np.linspace(u_min, u_max, 100), np.linspace(v_min, v_max, 100))\n",
    "    UV = np.stack([U.ravel(), V.ravel()]).T\n",
    "    c = prediction_func(UV).ravel()\n",
    "    c = c > 0.5\n",
    "    plt.scatter(UV[:, 0], UV[:, 1], c=c, edgecolors=\"none\", alpha=0.15)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y.ravel(), edgecolors=\"black\")\n",
    "    plt.xlim(left=u_min, right=u_max)\n",
    "    plt.ylim(bottom=v_min, top=v_max)\n",
    "    plt.axes().set_aspect(\"equal\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_model_prediction(lambda x: model.forward(x), X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
