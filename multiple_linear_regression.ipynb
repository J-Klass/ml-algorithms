{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "The goal is to find the linear function that best desribes miles-per-gallon for a car given the cylinder, horespower and weight of the car. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mpg.csv\"\n",
    ")\n",
    "cars = cars[[\"mpg\", \"cylinders\", \"horsepower\", \"weight\"]]\n",
    "cars = cars.dropna()\n",
    "cars = (cars - cars.mean()) / cars.std()\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "**Notation**\n",
    "\n",
    "* **n**:    Number of features\n",
    "* **m**:    Number of training examples\n",
    "* **x**:    Input variable / features\n",
    "* **y**:    Output variable / target\n",
    "\n",
    "\n",
    "\n",
    "**Hypothesis Function**\n",
    "\n",
    "$h_\\theta = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 + ... + \\theta_nx_n$\n",
    "\n",
    "By adding a row of ones to X everything can be written in matrix form.\n",
    "\n",
    "$$y = \\begin{bmatrix}y_0\\\\ y_1\\\\ .\\\\.\\\\ y_m\\end{bmatrix}  X = \\begin{bmatrix}1 & X_{1,1} & X_{1,2}  &  .&.& X_{1,n}\\\\ 1 & X_{2,1} & . &.  & . &.\\\\ .&.  &  .&  .&. &. \\\\ . &  .& . &.  &. & .\\\\ 1 & X_{m,1} & . &  .& .& X_{m,n}\\end{bmatrix} \\theta = \\begin{bmatrix}\\theta_0\\\\ \\theta_1\\\\ .\\\\ .\\\\ \n",
    "\\theta_m\\end{bmatrix}$$\n",
    "\n",
    "$$ y: m \\times 1 \\;\\;\\;\\; X: m \\times (n+1) \\;\\;\\;\\; \\theta: (n+1)\\times n $$\n",
    "\n",
    "The new **Hypothesis Function** is\n",
    "\n",
    "$h_\\theta = X\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Solution Analytically\n",
    "\n",
    "Calculate the RSS (residual sum of squares)\n",
    "\n",
    "$$ \\textrm{RSS} = (y-X\\theta)^T(y-X\\theta) $$\n",
    "$$ \\textrm{RSS} = (y^T-\\theta^TX^T)^T(y-X\\theta) $$\n",
    "$$ \\textrm{RSS} = y^Ty - y^TX\\theta - \\theta^TX^Ty + \\theta^TX^TX\\theta $$\n",
    "\n",
    "Minimizing RSS\n",
    "\n",
    "$$ \\frac{\\partial (\\textrm{RSS})}{\\partial \\theta} = \\frac{\\partial (y^Ty - y^TX\\theta - \\theta^TX^Ty + \\theta^TX^TX\\theta)}{\\partial \\theta} = 0 $$\n",
    "\n",
    "$$\\frac{\\partial (y^Ty)}{\\partial \\theta} -\\frac{\\partial (y^TX\\theta)}{\\partial \\theta} -\\frac{\\partial (\\theta^TX^Ty)}{\\partial \\theta} +\\frac{\\partial (\\theta^TX^TX\\theta)}{\\partial \\theta} = 0 $$\n",
    "$$ 0 - y^TX - X^Ty^T + 2\\theta^TX^TX = 0 $$\n",
    "$$ 2\\theta^TX^TX = 2y^TX$$\n",
    "$$ \\theta = (X^TX)^{-1}X^Ty $$\n",
    "\n",
    "The analytical solution does not scale well, as inverting a matrix has roughly complexity of $O(n^3)$, therefore numerical approximation methods are used for problems with a large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the best Parameters using Gradient Descent\n",
    "\n",
    "repeat until convergence \n",
    "\n",
    "$$ \\theta_j = \\theta_j - \\alpha\\frac{\\partial }{\\partial \\theta_j}J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = np.array(cars.drop([\"mpg\"], axis=1))\n",
    "y = np.array(cars[\"mpg\"])\n",
    "# Adding the intercept\n",
    "ones = np.ones([X.shape[0], 1])\n",
    "X = np.concatenate((ones, X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Example Analytically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving the example using the analytical solution\n",
    "hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "print(hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Example using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(x, y, theta):\n",
    "    # difference between f(x) and y output\n",
    "    dif = np.dot(x, theta) - y\n",
    "    cost = np.sum(dif ** 2) / (2 * np.shape(x)[0])\n",
    "    return dif, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_gradient_descent(x, y, theta, learning_rate, num_iterations):\n",
    "    cost_history = []\n",
    "    for i in range(num_iterations):\n",
    "        dif, cost = cost_function(x, y, theta)\n",
    "        gradient = np.dot(x.transpose(), dif) / np.shape(x)[0]\n",
    "        theta = theta - learning_rate * gradient\n",
    "        cost_history.append(cost)\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameter\n",
    "learning_rate = 0.0001\n",
    "num_iterations = 1000000\n",
    "\n",
    "# Initialize all weights as 0\n",
    "_, num_features = np.shape(X)\n",
    "initial_theta = np.zeros(num_features)\n",
    "theta, cost_history = multivariate_gradient_descent(\n",
    "    X, y, initial_theta, learning_rate, num_iterations\n",
    ")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(num_iterations), cost_history, \"r\")\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Cost\")\n",
    "ax.set_title(\"Error vs. Training Epoch\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
