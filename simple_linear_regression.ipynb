{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear (univariate) Regression<a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to find a linear function that fits the data below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 2.3, 1.5]\n",
    "y = [1, 2, 3, 2.1, 1.7]\n",
    "plt.plot(x, y, \"o\", color=\"blue\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "**Notation**\n",
    "\n",
    "* **m**:    Number of training examples\n",
    "* **x**:    Input variable / features\n",
    "* **y**:    Output variable / target\n",
    "\n",
    "**Hypothesis Function**\n",
    "\n",
    "$h_\\theta = \\theta_0 + \\theta_1 x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "$$J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}{(h_\\theta(x^{(i)})-y^{(i)})^2}$$\n",
    "\n",
    "The goal of the simple linear regression is to optimize the Parameters $\\theta_0, \\theta_1$ so that the cost is minimal. Different cost function are possible but the above illustraded MSE (mean squared error) is very common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing the Cost Function Analytically\n",
    "\n",
    "The Parameters $\\theta_0, \\theta_1$ can be calculated analytically. \n",
    "\n",
    "$$ \\arg\\min_{\\theta_0, \\theta_1}J(\\theta_0, \\theta_1) =  \\arg\\min_{\\theta_0, \\theta_1}\\frac{1}{2m}\\sum_{i=1}^{m}{(h_\\theta(x^{(i)})-y^{(i)})^2} = \\arg\\min_{\\theta_0, \\theta_1}\\frac{1}{2m}\\sum_{i=1}^{m}{(\\theta_0 + \\theta_1x^{(i)}-y^{(i)})^2}$$\n",
    "\n",
    "**Differentiating with respect to $\\theta_0$**\n",
    "\n",
    "$$ 2\\frac{1}{2m}\\sum_{i=1}^{m}{(\\theta_0 + \\theta_1x^{(i)}-y^{(i)})}(-1) = 0 $$\n",
    "\n",
    "\n",
    "$$ \\sum_{i=1}^{m}{(\\theta_0 + \\theta_1x^{(i)}-y^{(i)})} = 0 $$\n",
    "\n",
    "$$ \\theta_0 = \\frac{\\sum_{i=1}^{m}{(y^{(i)}-\\theta_1x^{(i)})}}{m} = \\sum_{i=1}^{m}{\\left(\\frac{y^{(i)}}{m}-\\theta_1\\frac{x^{(i)}}{m}\\right)} = \\sum_{i=1}^{m}{\\frac{y^{(i)}}{m}} - \\theta_1 \\sum_{i=1}^{m}{\\frac{x^{(i)}}{m}} $$\n",
    "\n",
    "\n",
    "**$$ \\theta_0 = \\bar{y} - \\theta_1 \\bar{x} $$**\n",
    "\n",
    "**Differentiating with respect to $\\theta_1$**\n",
    "\n",
    "$$ 2\\frac{1}{2m}\\sum_{i=1}^{m}{(\\theta_0 + \\theta_1x^{(i)}-y^{(i)})}(x^{(i)}) = 0 $$\n",
    "\n",
    "$$\\sum_{i=1}^{m}{(\\theta_0 + \\theta_1x^{(i)}-y^{(i)})}(x^{(i)}) = 0 $$\n",
    "\n",
    "$$ \\theta_0\\sum_{i=1}^{m}{x^{(i)}} + \\theta_1\\sum_{i=1}^{m}{\\left(x^{(i)}\\right)^2 - \\sum_{i=1}^{m}{(x^{(i)}y^{(i)}})} = 0 $$\n",
    "\n",
    "$$ \\left( \\sum_{i=1}^{m}{\\frac{y^{(i)}}{m}} - \\theta_1 \\sum_{i=1}^{m}{\\frac{x^{(i)}}{m}}\\right)\\sum_{i=1}^{m}{x^{(i)}} + \\theta_1\\sum_{i=1}^{m}{(x^{(i)})^2 - \\sum_{i=1}^{m}{(x^{(i)}y^{(i)}})} = 0$$\n",
    "\n",
    "$$ \\frac{1}{m}\\sum_{i=1}^{m}{y^{(i)}}\\sum_{i=1}^{m}{x^{(i)}} - \\theta_1 \\frac{1}{m}\\sum_{i=1}^{m}{x^{(i)}}\\sum_{i=1}^{m}{x^{(i)}} + \\theta_1\\sum_{i=1}^{m}{(x^{(i)})^2 - \\sum_{i=1}^{m}{(x^{(i)}y^{(i)}})} = 0$$\n",
    "\n",
    "$$ - \\theta_1 \\frac{1}{m}\\sum_{i=1}^{m}{x^{(i)}}\\sum_{i=1}^{m}{x^{(i)}} + \\theta_1\\sum_{i=1}^{m}{(x^{(i)})^2  = \\sum_{i=1}^{m}{(x^{(i)}y^{(i)}})} - \\frac{1}{m}\\sum_{i=1}^{m}{y^{(i)}}\\sum_{i=1}^{m}{x^{(i)}} $$ \n",
    "\n",
    "$$\\theta_1=\\frac{\\sum_{i=1}^{m}{(x^{(i)}y^{(i)})} - \\frac{1}{m} \\sum_{i=1}^{m}{y^{(i)}} \\sum_{i=1}^{m}{x^{(i)}}}{  \\sum_{i=1}^{m}{(x^{(i)})^2} -\\frac{1}{m}(\\sum_{i=1}^{m}{x^{(i)}})^2 }$$\n",
    "\n",
    "$$\\theta_1=\\frac{\\sum_{i=1}^{m}{(x^{(i)}y^{(i)})} - m\\bar{y}\\bar{x}}{  \\sum_{i=1}^{m}{(x^{(i)})^2} -m \\bar{x}^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the best coefficients analytically\n",
    "\n",
    "# Calculating theta 1\n",
    "numerator = np.sum((x - np.mean(x)) * (y - np.mean(y)))\n",
    "denominator = np.sum((x - np.mean(x)) ** 2)\n",
    "theta_1 = numerator / denominator\n",
    "\n",
    "# Calculating theta 0\n",
    "theta_0 = np.mean(y) - theta_1 * np.mean(x)\n",
    "\n",
    "\n",
    "def linear_regression_analytically(x):\n",
    "    return theta_0 + theta_1 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the linear regression\n",
    "y_pred = [linear_regression_analytically(i) for i in x]\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_pred, color=\"red\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
